## 时间安排：

- [x] 2021.7.20:确定参与人员（候选浙江理工，北林，成电组）
  - [x] 成电，罗仁威；北林两人。
- [x] 2021.7.27 周二，实验前期准备，确定并行框架
  - [x] 已有并行算法环境+cartpole 的实验环境，介绍目前现在代码框架，以及初步实验结果。（罗仁威）
  - [x] 熟悉 parl 框架，运行几个例子程序。（巴悦浩，胡雪莲）
  - [x] 在parl框架下，按照之前论文设计恶意进程的行为（张明悦）
- [ ] 2021.8.10 周二: 第一阶段实验 
  - [x] a3c+hardMAB算法，在cartpole跑实验（张明悦）
  - [x] a3c+hardMAB算法，改到 BipedalWalker-v2或其他的场景的任务（罗仁威）
  - [x] parl 框架的并行化的代码（巴悦浩，胡雪莲）
- [ ] 2021.8.20:论文
- [ ] 2021.8.21:

论文进度：**10月1日 abstract** **10月8日 paper**

- [ ] 9.6-9.12，实验，为什么不work；

- [ ] 9.13-9.19，实验，以及论文实验部分写作；
- [ ] 9.20-9.26，实验数据整理，论文方法部分写作；
- [ ] 9.27-10.1，论文引言，正文部分。

## 思路1: hard 采样模式

把并行rl考虑成combinational multi-armed bandits问题，组合老虎机。

单个actor，就是一个臂，n个actor，0是不选择，1是选择，[0,0,1..,0]的向量 $v$ 就是actor选择向量。

确定最优的 $v$，保证好的actor都被选择，坏的actor都不被选择。

论文：

[1]Learning multiuser channel allocations in cognitive radio networks: A combinatorial multi-armed bandit formulation

**[2] Combinatorial multi-armed bandit: General framework and applications，icml2013**

[3] Combinatorial Multi-armed Bandits for Real-Time Strategy Games

主要基于[2]的算法：

<img src="2-pic/cmab.png" alt="cmab" style="zoom:40%;" />

对应到并行RL中：

**算法1**

参数 K 好节点的数量。

（1）每个actor i，都有置信度Qi，Qi先都初始化为0；$\mu_i$ 全初始化。

（2）从所有actor中，选择 $\mu{_i}$ 值前K个actor进入到（supper bandit） S中；

（3）play S，即用 S 中的 actor运行的到环境奖励；因为只有一个环境奖励，于是这句，S中的所有actor的reward 都为 r。

（4）对于S中的 actor $Q_i=Q_i+\alpha[r-Q_i]$

（5） 对于每个actor，$\mu_{i}=Q_i+\sqrt{\frac{3lnt}{2T_i}}$, 其中 t 是一共的运轮数，$T_i$ 是 actor i 被选到的次数。

（6）如果没有到达最大的t轮数，就跳转到（2）执行。

**算法2**

e-greedy：K 是好的actor。

（1）actor Qi=[0,....0]

 (2) e: 随机选择actor 选择K个；actor-》分数；

1-e概率：选择Qi top K。；actor-〉分数；

（3）被选择的actor Qi=Qi+a(R-Qi)

(4)回到1.

**算法3**

为了提升Red A3C效果。actor [0,...,i,...,n]，0号worker（learner）是好worker专门维护全局置信度Q=[Q0,...,Qi,...,Qn]。事先知道好worker有k个（包括0worker），坏的有n-k个。

根据Q，0worker计算出worker-set=[0,1,3,....]， worker-set有k个节点，一定会包含0节点。根据Q计算worker-set的方式可以为，ucb，softmax，e-greedy等。

只有worker-set中的worker执行，执行若干个episode；（注意这时是异步执行）由0worker计算Q。

运行一段时间后，0worker终止worker-set中的worker，0worker根据Q再次选择worker-set。

【0，1，2，3，4】+【5，6】

【0，2，3，4，5】-》异步更新policy网络，value网络。10epsiode；

当前 0worker policy，在环境中运行得到 R。R分给【0，2，3，4，5】。







# 实验

**baseline算法：**

多线程的rl（基于A3C）

A3C+hardMAB（我们的算法）

A3C+softMAB（我们的第二个算法）

A3C+去掉得分低的actor

纯PRL（没有筛选坏节点的算法）

分布式的rl（基于parl）

a2c（https://openai.com/blog/baselines-acktr-a2c/）（ Asynchronous methods for deep reinforcement learning.）





**e1-benchmark：gym**

https://gym.openai.com/

选5个实验场景：

atari (Alien，BeamRider，Pong，Breakout，Qbert，RoadRunner)，

box2D（BipedalWalker-v2），Classic control（CartPole-v1）。

**e2 一个实际的分布式rl场景？？**



# 重要材料

rllib

https://docs.ray.io/en/master/rllib.html

parl

https://github.com/PaddlePaddle/PARL

python多线程

https://docs.python.org/zh-cn/3/library/multiprocessing.html

https://mofanpy.com/tutorials/python-basic/multiprocessing/why/



A3C：Asynchronous Methods for Deep Reinforcement Learning. 2016 由 Deepmind 提出.

A2C 没有具体的论文，是 OpenAI 在实现 baseline 中发现多个actor的梯度同步更新或异步更新产生的效果差不多，所以写了一个博客 [https://openai.com/blog/baselines-acktr-a2c/](https://link.zhihu.com/?target=https%3A//openai.com/blog/baselines-acktr-a2c/)， 同时在 OpenAI baseline [https://github.com/openai/baselines/tree/master/baselines/a2c](https://link.zhihu.com/?target=https%3A//github.com/openai/baselines/tree/master/baselines/a2c) 中实现了A2C.

AC （actor-critic）是 sutton 在 2000 年写的论文中提出的：Policy Gradient Methods for Reinforcement Learning with Function Approximation，链接：[https://homes.cs.washington.edu]

https://link.zhihu.com/?target=https%3A//homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf





# 代码备忘

lrw resuse_MAB代码：Global_credit 都是 0；我的原始代码ours_a3c也是这个情况。

目前Red A3C选不出最优worker的原因：有push-pull的操作，坏的worker也从共享参数中拉网络参数下来。因此，坏worker也会表现出好的行为。看选的worker没有太大意义，只能看跑出来的得分。

